import os
import json
import asyncio
from groq import Groq
from typing import Dict, Any
import google.generativeai as genai

# Global variable to switch between LLM providers
# Options: "groq" or "gemini"
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "gemini").lower()

# Initialize Groq client
groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# Initialize Gemini client
genai.configure(api_key=os.getenv("GEMINI_API_KEY", "AIzaSyDNYzLRm4coQ1krriVjeci1nprOr4c2KTA"))

# Estimate ~4 characters per token (rough approximation)
MAX_INPUT_CHARS = 15000  # ~2000 tokens for input
SYSTEM_PROMPT_TOKENS = 400  # Reserve space for system prompt
RESERVE_TOKENS = 500  # Reserve for response + safety margin

def truncate_data(raw_data: str, target_schema: str) -> str:
    """
    Intelligently truncate raw data to fit within Groq's token limits.
    Keeps beginning and end of data to preserve context.
    """
    if isinstance(raw_data, dict):
        # If raw_data is already a dict (from JSON parsing), convert to string
        raw_data = json.dumps(raw_data)
    
    data_str = str(raw_data)
    
    # Rough token estimation: ~4 chars per token
    schema_tokens = len(target_schema) // 4
    # Max tokens for llama-3 is around 8k or 32k depending on version, keeping safe at 6k chars effective context
    available_chars = 15000 # Decreased to avoid 429 Rate Limit error on free tier
    
    if len(data_str) <= available_chars:
        return data_str
    
    print(f"[WARNING] Input data too large ({len(data_str)} chars). Truncating to {available_chars} chars...")
    
    # Strategy: Keep first 60% and last 40% of data to preserve context
    keep_first = int(available_chars * 0.6)
    keep_last = int(available_chars * 0.4)
    
    truncated = (
        data_str[:keep_first] + 
        f"\n\n[... {len(data_str) - available_chars} characters omitted ...]\n\n" + 
        data_str[-keep_last:]
    )
    
    return truncated

async def ai_structurizer(raw_data: str, target_schema: str) -> Dict[str, Any]:
    """
    Uses Groq or Gemini LLM to clean, normalize, and structure raw multimedia data.
    Handles large inputs by smart truncation.
    Switches provider based on LLM_PROVIDER global variable.
    """
    
    # Truncate if necessary
    processed_data = truncate_data(raw_data, target_schema)
    
    system_prompt = f"""
    You are an expert Data Engineer AI. Your task is to extract information from messy, 
    unstructured multimedia data and convert it into a valid, clean JSON object.

    ### TARGET SCHEMA:
    {target_schema}

    ### RULES:
    1. STRUCTURE: Only return valid JSON. No conversational text.
    2. CLEANING: Remove noise, boilerplate text, and irrelevant characters.
    3. NORMALIZATION: 
       - Dates: Convert to YYYY-MM-DD.
       - Currency: Convert to numeric float (e.g., "$1,200.50" -> 1200.50).
       - Strings: Proper casing (Title Case for names).
    4. ERROR HANDLING: If a field in the schema cannot be found, set it to null.
    5. MULTILINGUAL: Translate any non-English data to English during extraction.
    6. PARTIAL DATA: If input is truncated, extract what you can from available data.
    """

    try:
        loop = asyncio.get_event_loop()
        
        if LLM_PROVIDER == "gemini":
            return await _call_gemini(processed_data, system_prompt)
        else:  # Default to groq
            return await _call_groq(processed_data, system_prompt)

    except json.JSONDecodeError as e:
        print(f"JSON parsing error: {str(e)}")
        return {"error": "LLM returned invalid JSON. Retrying may be necessary."}
    except Exception as e:
        error_msg = str(e)
        print(f"LLM API Error: {error_msg}")
        
        # Check if it's a rate limit/token size error
        if "too large" in error_msg.lower() or "rate_limit" in error_msg.lower():
            return {"error": "Input data exceeded token limit. Please upload a smaller file or reduce the amount of data."}
        
        return {"error": f"LLM API Error: {error_msg}"}


async def _call_groq(processed_data: str, system_prompt: str) -> Dict[str, Any]:
    """
    Call Groq API for data structuring.
    """
    loop = asyncio.get_event_loop()
    
    def call_groq_api():
        return groq_client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"RAW DATA TO PROCESS:\n{processed_data}"}
            ],
            response_format={"type": "json_object"},
            temperature=0.1,
        )
    
    chat_completion = await loop.run_in_executor(None, call_groq_api)
    structured_output = json.loads(chat_completion.choices[0].message.content)
    return structured_output


async def _call_gemini(processed_data: str, system_prompt: str) -> Dict[str, Any]:
  
    loop = asyncio.get_event_loop()
    
    def call_gemini_api():
        model = genai.GenerativeModel(
            model_name="gemini-2.5-flash-lite",
            system_instruction=system_prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.1,
                response_mime_type="application/json"
            )
        )
        
        response = model.generate_content(f"RAW DATA TO PROCESS:\n{processed_data}")
        return response.text
    
    json_response = await loop.run_in_executor(None, call_gemini_api)
    structured_output = json.loads(json_response)
    return structured_output


async def ai_structurizer_to_csv(raw_data: str, target_schema: str, is_csv_input: bool = False) -> str:
    """
    Directly converts raw data to CSV format without going through JSON intermediate.
    Optimized for CSV inputs and faster processing.
    
    Args:
        raw_data: The raw unstructured/structured data to process
        target_schema: Description of desired output columns/structure
        is_csv_input: If True, preserves original CSV columns and adds requested fields
    
    Returns:
        String containing CSV data
    """
    
    # Truncate if necessary
    processed_data = truncate_data(raw_data, target_schema)
    
    # Build appropriate prompt based on input type
    if is_csv_input:
        system_prompt = f"""
    You are an expert Data Engineer AI. Your task is to process CSV data and convert it into 
    a clean, enhanced CSV format.

    ### TARGET SCHEMA/REQUIREMENTS:
    {target_schema}

    ### RULES FOR CSV INPUT:
    1. **PRESERVE ORIGINAL COLUMNS**: Keep all columns from the input CSV
    2. **ADD NEW FIELDS**: If the target schema requests additional fields, add them as new columns
    3. **CLEANING**: Remove noise, fix formatting issues, standardize data
    4. **NORMALIZATION**: 
       - Dates: Convert to YYYY-MM-DD
       - Currency: Convert to numeric float (e.g., "$1,200.50" -> 1200.50)
       - Strings: Proper casing (Title Case for names)
    5. **ERROR HANDLING**: If a new field cannot be extracted, leave it empty
    6. **HEADER ROW**: First row must be column headers
    7. **QUOTING**: Properly quote values that contain commas or special characters
    8. **OUTPUT**: Return ONLY the CSV content (including header row), no explanations
    
    ### EXAMPLE:
    Input CSV:
    name,age
    john,25
    jane,30
    
    Target Schema: "Add email and city columns"
    
    Output CSV:
    "Name","Age","Email","City"
    "John","25","","" 
    "Jane","30","",""
    
    Return ONLY the CSV content, nothing else.
    """
    else:
        system_prompt = f"""
    You are an expert Data Engineer AI. Your task is to extract information from messy, 
    unstructured data and convert it DIRECTLY into a clean CSV file.

    ### TARGET SCHEMA:
    {target_schema}

    ### RULES:
    1. **STRUCTURE**: Only return valid CSV. No conversational text, no JSON.
    2. **CLEANING**: Remove noise, boilerplate text, and irrelevant characters
    3. **NORMALIZATION**: 
       - Dates: Convert to YYYY-MM-DD
       - Currency: Convert to numeric float (e.g., "$1,200.50" -> 1200.50)
       - Strings: Proper casing (Title Case for names)
    4. **ERROR HANDLING**: If a field cannot be found, leave it empty in the CSV
    5. **MULTILINGUAL**: Translate any non-English data to English during extraction
    6. **PARTIAL DATA**: If input is truncated, extract what you can from available data
    7. **ARRAY NORMALIZATION**: If there are arrays/lists, create ONE ROW per array element
    8. **HEADER ROW**: First row must be clear, readable column names
    9. **QUOTING**: Properly quote values that contain commas or special characters
    10. **NO DUPLICATES**: Ensure no duplicate rows
    
    ### EXAMPLE:
    Input: "Invoice #123: John Doe bought Product A for $100 on 2024-01-01. Jane Smith bought Product B for $200 on 2024-01-02."
    Schema: "Extract invoice number, customer name, product, price, date"
    
    Output CSV:
    "Invoice Number","Customer Name","Product","Price","Date"
    "123","John Doe","Product A","100.00","2024-01-01"
    "","Jane Smith","Product B","200.00","2024-01-02"
    
    Return ONLY the CSV content, nothing else.
    """
    
    try:
        loop = asyncio.get_event_loop()
        
        if LLM_PROVIDER == "gemini":
            return await _call_llm_for_direct_csv_gemini(processed_data, system_prompt)
        else:  # Default to groq
            return await _call_llm_for_direct_csv_groq(processed_data, system_prompt)

    except Exception as e:
        error_msg = str(e)
        print(f"Direct CSV Conversion Error: {error_msg}")
        
        # Fallback: return raw data as single-column CSV
        return f'"Raw Data"\n"{str(raw_data)[:1000].replace(chr(34), chr(34)+chr(34))}"'


async def _call_llm_for_direct_csv_groq(processed_data: str, system_prompt: str) -> str:
    """
    Call Groq API for direct CSV conversion from raw data.
    """
    loop = asyncio.get_event_loop()
    
    def call_groq_api():
        return groq_client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"RAW DATA TO PROCESS:\n{processed_data}"}
            ],
            temperature=0.1,
        )
    
    chat_completion = await loop.run_in_executor(None, call_groq_api)
    csv_content = chat_completion.choices[0].message.content.strip()
    
    # Remove markdown code fences if present
    if csv_content.startswith("```"):
        lines = csv_content.split("\n")
        csv_content = "\n".join(lines[1:-1]) if len(lines) > 2 else csv_content
    
    return csv_content


async def _call_llm_for_direct_csv_gemini(processed_data: str, system_prompt: str) -> str:
    """
    Call Gemini API for direct CSV conversion from raw data.
    """
    loop = asyncio.get_event_loop()
    
    def call_gemini_api():
        model = genai.GenerativeModel(
            model_name="gemini-2.5-flash-lite",
            system_instruction=system_prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.1
            )
        )
        
        response = model.generate_content(f"RAW DATA TO PROCESS:\n{processed_data}")
        return response.text
    
    csv_content = await loop.run_in_executor(None, call_gemini_api)
    csv_content = csv_content.strip()
    
    # Remove markdown code fences if present
    if csv_content.startswith("```"):
        lines = csv_content.split("\n")
        csv_content = "\n".join(lines[1:-1]) if len(lines) > 2 else csv_content
    
    return csv_content


# async def json_to_csv_converter(json_data: Dict[str, Any]) -> str:
#     """
#     Uses Groq or Gemini LLM to intelligently convert JSON data to CSV format.
#     Handles nested structures, arrays, and complex data normalization.
    
#     Args:
#         json_data: The JSON data to convert to CSV
    
#     Returns:
#         String containing the CSV data
#     """
    
#     # Convert JSON to string for LLM input
#     json_str = json.dumps(json_data, indent=2)
    
#     system_prompt = """
#     You are an expert Data Engineer AI specialized in CSV generation. Your task is to convert 
#     JSON data into a clean, well-formatted CSV file.

#     ### RULES:
#     1. **Array Normalization**: If there are arrays of objects (e.g., "eras": [...]), create ONE ROW per array element
#     2. **Column Structure**: Extract fields from objects as separate columns
#     3. **Header Formatting**: Use clear, readable column names (e.g., "Best Player" not "best_player")
#     4. **Nested Arrays**: Convert to semicolon-separated values (e.g., ["a", "b"] becomes "a; b")
#     5. **Missing Values**: Use empty strings for missing/null values
#     6. **No Duplicates**: Ensure no duplicate rows
#     7. **Format**: Return ONLY the CSV content (including header row), nothing else
#     8. **Quoting**: Properly quote values that contain commas or special characters

#     ### EXAMPLE:
#     Input JSON:
#     {
#       "eras": [
#         {"name": "Era1", "player": "John", "teams": ["A", "B"]},
#         {"name": "Era2", "player": "Jane", "teams": ["C"]}
#       ]
#     }

#     Expected CSV Output:
#     "Name","Player","Teams"
#     "Era1","John","A; B"
#     "Era2","Jane","C"

#     Return ONLY the CSV content, no additional text or explanations.
#     """
    
#     try:
#         loop = asyncio.get_event_loop()
        
#         if LLM_PROVIDER == "gemini":
#             csv_result = await _call_llm_for_csv_gemini(json_str, system_prompt)
#         else:  # Default to groq
#             csv_result = await _call_llm_for_csv_groq(json_str, system_prompt)
        
#         return csv_result
        
#     except Exception as e:
#         error_msg = str(e)
#         print(f"CSV Conversion Error: {error_msg}")
#         # Fallback: return JSON as single-column CSV
#         return f"\"Data\"\n\"{json_str.replace(chr(34), chr(34)+chr(34))}\""

async def _call_llm_for_csv_groq(json_str: str, system_prompt: str) -> str:
    """
    Call Groq API for JSON to CSV conversion.
    """
    loop = asyncio.get_event_loop()
    
    def call_groq_api():
        return groq_client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Convert this JSON to CSV:\\n{json_str}"}
            ],
            temperature=0.1,
        )
    
    chat_completion = await loop.run_in_executor(None, call_groq_api)
    csv_content = chat_completion.choices[0].message.content.strip()
    
    # Remove markdown code fences if present
    if csv_content.startswith("```"):
        lines = csv_content.split("\n")
        csv_content = "\n".join(lines[1:-1]) if len(lines) > 2 else csv_content
    
    return csv_content


async def _call_llm_for_csv_gemini(json_str: str, system_prompt: str) -> str:
    """
    Call Gemini API for JSON to CSV conversion.
    """
    loop = asyncio.get_event_loop()
    
    def call_gemini_api():
        model = genai.GenerativeModel(
            model_name="gemini-2.5-flash-lite",
            system_instruction=system_prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.1
            )
        )
        
        response = model.generate_content(f"Convert this JSON to CSV:\\n{json_str}")
        return response.text
    
    csv_content = await loop.run_in_executor(None, call_gemini_api)
    csv_content = csv_content.strip()
    
    # Remove markdown code fences if present
    if csv_content.startswith("```"):
        lines = csv_content.split("\n")
        csv_content = "\n".join(lines[1:-1]) if len(lines) > 2 else csv_content
    
    return csv_content


async def generate_data_autopsy_report(processing_data: Dict[str, Any]) -> str:
    """
    Generate a comprehensive "Data Autopsy" style report using Groq LLM.
    Includes error analysis, data health metrics, lineage tracking, and AI readiness badges.
    
    Args:
        processing_data: The complete processing results and metadata
    
    Returns:
        Formatted markdown report
    """
    
    # Convert processing data to JSON string for LLM
    data_str = json.dumps(processing_data, indent=2)
    
    system_prompt = """
    You are an elite Data Forensics AI specialized in creating comprehensive "Data Autopsy" reports.
    Your reports are legendary for their clarity, insights, and actionable recommendations.

    ## YOUR MISSION:
    Analyze the data processing results and create a professional, visually-striking report that includes:

    ### 1. üìä DATA AUTOPSY (Error Root Cause Analysis)
    - Identify ALL error patterns and their distribution percentages
    - Explain WHY each type of error occurred
    - Group errors by category (format issues, missing data, validation failures, etc.)
    - Example format:
      ```
      üîç ERROR DISTRIBUTION:
      ‚Ä¢ 63% - Date format mismatches (DD/MM/YYYY vs ISO)
      ‚Ä¢ 21% - Missing required fields (email, phone)
      ‚Ä¢ 11% - Duplicate entries detected
      ‚Ä¢ 5% - Unknown pattern anomalies
      ```

    ### 2. üíä DATA HEALTH SCORE
    - Calculate overall data health (0-100 scale)
    - Show before/after if cleaning was applied
    - Break down by categories (completeness, accuracy, consistency, validity)
    
    ### 3. üéØ AI READINESS BADGES
    Award badges based on data quality:
    - ‚úÖ ML READY (90-100 health score)
    - ‚úÖ ANALYTICS READY (70-89 health score)
    - ‚ö†Ô∏è NEEDS REVIEW (50-69 health score)
    - ‚ùå CRITICAL ISSUES (<50 health score)

    ### 4. üß¨ DATA LINEAGE SUMMARY
    - Show transformation pipeline stages
    - Highlight what changed at each step
    - Track data flow: Raw ‚Üí Cleaned ‚Üí Normalized ‚Üí Validated

    ### 5. üí° KEY INSIGHTS (ELI5 Mode)
    - Provide both technical AND plain English explanations
    - Example: "Your data is messy because dates are written in different styles (some use MM/DD/YYYY, others DD/MM/YYYY)"

    ### 6. üöÄ RECOMMENDATIONS
    - Actionable next steps to improve data quality
    - Specific fixes that would have the highest impact
    - "What if I fix this?" scenarios with projected improvements

    ### 7. üìà STATISTICS SUMMARY
    - Total records processed
    - Success rate
    - Processing time (if available)
    - Data size before/after

    ## FORMATTING RULES:
    - Use Markdown with emojis for visual appeal
    - Use bullet points, tables, and sections liberally
    - Include percentages and specific numbers
    - Make it scannable with clear headers
    - Use technical terms but also provide plain English
    - Be confident and insightful, not generic

    ## OUTPUT FORMAT:
    Return ONLY the markdown report. No preamble, no "Here's your report", just the content.
    """
    
    try:
        loop = asyncio.get_event_loop()
        
        def call_groq_api():
            return groq_client.chat.completions.create(
                model="openai/gpt-oss-120b",  # Using the most capable model
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Generate a comprehensive Data Autopsy report for this processing data:\n\n{data_str}"}
                ],
                temperature=0.3,  # Slightly higher for more creative insights
                max_tokens=4000,  # Allow detailed report
            )
        
        chat_completion = await loop.run_in_executor(None, call_groq_api)
        report_content = chat_completion.choices[0].message.content.strip()
        
        # Remove markdown code fences if present
        if report_content.startswith("```"):
            lines = report_content.split("\n")
            report_content = "\n".join(lines[1:-1]) if len(lines) > 2 else report_content
        
        return report_content
        
    except Exception as e:
        error_msg = str(e)
        print(f"Report Generation Error: {error_msg}")
        
        # Fallback: create basic report
        return f"""# Data Processing Report

## Summary
Processing completed. See JSON output for detailed results.

**Total Sources:** {len(processing_data.get('results', []))}
**Status:** Completed

**Error:** Unable to generate detailed autopsy report ({error_msg})

Please review the JSON output for complete data.
"""